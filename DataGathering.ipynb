{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fd30b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from neurostatx.utils.preprocessing import merge_dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e00ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_path = \"c:/Users/Rosalie/OneDrive - Office 365/Documents/UdeS/Hiver 2026/Crédits de recherche/GitHub/FuzzyClustering-PING/\"\n",
    "ping_base_path = \"c:/Users/Rosalie/OneDrive - Office 365/Documents/UdeS/Hiver 2026/Crédits de recherche/data/PINGTabularData/\"\n",
    "ping_int_path = \"c:/Users/Rosalie/OneDrive - Office 365/Documents/UdeS/Hiver 2026/Crédits de recherche/result/inversenormaltransformation/\"\n",
    "output_dir = \"c:/Users/Rosalie/OneDrive - Office 365/Documents/UdeS/Hiver 2026/Crédits de recherche/result/datagathering/\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3cde67",
   "metadata": {},
   "source": [
    "Fetching relevent data from PING study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e0c79d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rosalie\\AppData\\Local\\Temp\\ipykernel_11260\\2955698399.py:12: DtypeWarning: Columns (0,1,2,6,10,11,13,14,16,17,18,19,20,21,22,24,25,26,27,28,29,30,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,54,55,56,57,58,59,60,68,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  demo = pd.read_csv(f\"{ping_base_path}/fdh01.txt\", sep=\"\\t\")\n"
     ]
    }
   ],
   "source": [
    "# Load all necessary dataframes for cognitive test\n",
    "dccs_int = pd.read_excel(f\"{ping_int_path}/dccs_int.xlsx\")\n",
    "flanker_int = pd.read_excel(f\"{ping_int_path}/flanker_int.xlsx\")\n",
    "ibam_int = pd.read_excel(f\"{ping_int_path}/ibam_int.xlsx\")\n",
    "lswmt_int = pd.read_excel(f\"{ping_int_path}/lswmt_int.xlsx\")\n",
    "orrt_int = pd.read_excel(f\"{ping_int_path}/orrt_int.xlsx\")\n",
    "pcps_int = pd.read_excel(f\"{ping_int_path}/pcps_int.xlsx\")\n",
    "tpvt_int = pd.read_excel(f\"{ping_int_path}/tpvt_int.xlsx\")\n",
    "\n",
    "# Load all necessary dataframes for covariates/demographics\n",
    "# Age, Sex, Ethnicity, income, education, ADHD, handedness\n",
    "demo = pd.read_csv(f\"{ping_base_path}/fdh01.txt\", sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd152fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rosalie\\AppData\\Local\\Temp\\ipykernel_11260\\2887618889.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  age[\"age_months\"] = pd.to_numeric(demo_base[\"interview_age\"], errors=\"coerce\")\n"
     ]
    }
   ],
   "source": [
    "demo_base = demo.iloc[1:].copy()\n",
    "\n",
    "# Extracting age data\n",
    "age = demo_base[[\"subjectkey\", \"interview_age\"]]\n",
    "age.columns = [\"subjectkey\", \"age_months\"]\n",
    "age[\"age_months\"] = pd.to_numeric(demo_base[\"interview_age\"], errors=\"coerce\")\n",
    "\n",
    "# Extracting demographics data\n",
    "demo_vars = [\n",
    "    \"subjectkey\",\n",
    "    \"sex\",\n",
    "    \"behavobs_handedness\",\n",
    "    \"race_w\",\n",
    "    \"race_b\", \n",
    "    \"ethnicity\",\n",
    "    \"race_othera\",\n",
    "    \"race_ia\",\n",
    "    \"race_otherpi\",\n",
    "    \"fdh_grdian_1_edu\",\n",
    "    \"fdh_grdian_2_edu\",\n",
    "    \"fdh_3_hhold_income\"\n",
    "]\n",
    "\n",
    "demo_data = demo_base[demo_vars].copy()\n",
    "demo_data.columns = [\n",
    "    \"subjectkey\",\n",
    "    \"sex\",\n",
    "    \"handedness\",\n",
    "    \"eth_white\",\n",
    "    \"eth_black\",\n",
    "    \"eth_hispanic_latino\",\n",
    "    \"eth_asia\",\n",
    "    \"eth_indian_ame\",\n",
    "    \"eth_pacific_islander\",\n",
    "    \"parent_ed1\",\n",
    "    \"parent_ed2\",\n",
    "    \"income\"\n",
    "]\n",
    "\n",
    "#  Transform the handedness score.  1 = left, 2 = right, 3 = ambidex\n",
    "def transform_handedness(x):\n",
    "    if x == \"L\":\n",
    "        return 1\n",
    "    elif x == \"R\":\n",
    "        return 2\n",
    "    elif x == \"A\":\n",
    "        return 3\n",
    "    else :\n",
    "        return np.nan\n",
    "\n",
    "demo_data.loc[:, \"handedness\"] = demo_data[\"handedness\"].apply(transform_handedness)\n",
    "\n",
    "# Transform sex data. 1 = male, 2 = female, 0 = unknown\n",
    "def transform_sex(x):\n",
    "    if x == \"M\":\n",
    "        return 1\n",
    "    elif x == \"F\":\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "demo_data.loc[:, \"sex\"] = demo_data[\"sex\"].apply(transform_sex)\n",
    "\n",
    "# Extract parental highest education level. Taking the highest among the two parents.\n",
    "for col in [\"parent_ed1\", \"parent_ed2\"]:\n",
    "    demo_data[col] = (\n",
    "        demo_data[col]\n",
    "        .replace([777, 999, \"Missing value\", \"Other\", np.nan], 0)\n",
    "    )\n",
    "    demo_data[col] = pd.to_numeric(demo_data[col], errors=\"coerce\")\n",
    "demo_data.loc[:, \"high_edu\"] = demo_data[[\"parent_ed1\", \"parent_ed2\"]].fillna(0).max(axis=1)\n",
    "\n",
    "# Create high parent education groups. 1 = no high school, 2 = high school, 3 = some college, 4 = bachelor, 5 = post-graduate, 0 = unknown\n",
    "def create_edu_groups(x):\n",
    "    if x < 4:\n",
    "        return 1 # no high school\n",
    "    elif x == 4:\n",
    "        return 2 # high school\n",
    "    elif x == 5:\n",
    "        return 3 # some college\n",
    "    elif x == 6: \n",
    "        return 4 # bachelor\n",
    "    elif x == 7 :\n",
    "        return 5  # post-graduate\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "demo_data.loc[:, \"edu_groups\"] = demo_data[\"high_edu\"].apply(create_edu_groups)\n",
    "\n",
    "# Regrouping ethnicity\n",
    "cols = [\n",
    "    \"eth_white\",\n",
    "    \"eth_black\",\n",
    "    \"eth_asia\",\n",
    "    \"eth_indian_ame\",\n",
    "    \"eth_pacific_islander\"\n",
    "]\n",
    "\n",
    "# Convert to numeric\n",
    "demo_data[cols] = demo_data[cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Recode 1 = yes, 0 = no\n",
    "demo_data[cols] = (demo_data[cols] == 1).astype(int)\n",
    "\n",
    "# Create a new column with the combined ethnicity categories. 1 = White, 2 = Black, 3 = Hispanic/Latino, 4 = Asian, 5 = Multiethnic, 6 = Other, 0 = Unknown\n",
    "def combine_ethnicity(row):\n",
    "    race_flags = [\n",
    "        row[\"eth_white\"],\n",
    "        row[\"eth_black\"],\n",
    "        row[\"eth_asia\"],\n",
    "        row[\"eth_indian_ame\"],\n",
    "        row[\"eth_pacific_islander\"]\n",
    "    ]\n",
    "    race_count = sum(race_flags)\n",
    "    is_hispanic = str(row[\"eth_hispanic_latino\"]).strip().lower() == \"hispanic or latino\"\n",
    "\n",
    "    # Multiethnic \n",
    "    if is_hispanic and race_count >= 1:\n",
    "        return 5  # Multiethnic\n",
    "    if race_count > 1:\n",
    "        return 5  # Multiethnic\n",
    "\n",
    "    # Hispanic only\n",
    "    if is_hispanic:\n",
    "        return 3\n",
    "\n",
    "    # Etnicity categories\n",
    "    if row[\"eth_white\"] == 1:\n",
    "        return 1 # White\n",
    "    elif row[\"eth_black\"] == 1:\n",
    "        return 2 # Black\n",
    "    elif row[\"eth_asia\"] == 1:\n",
    "        return 4 # Asian\n",
    "    elif row[\"eth_indian_ame\"] == 1 or row[\"eth_pacific_islander\"] == 1:\n",
    "        return 6  # Other\n",
    "    else:\n",
    "        return 0  # Unknown\n",
    "\n",
    "demo_data[\"combine_ethnicity\"] = demo_data.apply(combine_ethnicity, axis=1)\n",
    "\n",
    "# Create new income groups into 1 = <50 000, 2 = 50 000 - 99 999, 3 = 100 000 +, 0 = unknown\n",
    "demo_data[\"income\"] = (demo_data[\"income\"].replace([\"Missing value\", 777, 999, np.nan], 0))\n",
    "demo_data[\"income\"] = pd.to_numeric(demo_data[\"income\"], errors=\"coerce\")\n",
    "\n",
    "def create_income_groups(x):\n",
    "    if pd.isna(x):\n",
    "        return 0\n",
    "    if x < 7:\n",
    "        return 1 # <50 000\n",
    "    elif x == 7 :\n",
    "        return 2 # 50 000 - 99 999\n",
    "    elif x in [8, 9, 10, 11, 12]:\n",
    "        return 3 # 100 000 +\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "demo_data.loc[:, \"income_groups\"] = demo_data[\"income\"].apply(create_income_groups)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de613c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy baseline data for each cognitive test\n",
    "dccs_base = dccs_int.iloc[1:].copy()\n",
    "flanker_base = flanker_int.iloc[1:].copy()\n",
    "ibam_base = ibam_int.iloc[1:].copy()\n",
    "lswmt_base = lswmt_int.iloc[1:].copy()\n",
    "orrt_base = orrt_int.iloc[1:].copy()\n",
    "pcps_base = pcps_int.iloc[1:].copy()\n",
    "tpvt_base = tpvt_int.iloc[1:].copy()\n",
    "\n",
    "# Create a function to scale the scores for each test\n",
    "def scale_test(df, original_col, new_col):\n",
    "    df = df[[\"subjectkey\", original_col]].copy()\n",
    "    df.columns = [\"subjectkey\", new_col]\n",
    "    df.loc[:, new_col] = StandardScaler().fit_transform(df[[new_col]])\n",
    "    return df\n",
    "\n",
    "# Scale scores for each test using the function\n",
    "dccs_base    = scale_test(dccs_base,    \"cardsort_scaled_score\",    \"card_sort\")\n",
    "flanker_base = scale_test(flanker_base, \"flanker_scaled_score\",     \"flanker\")\n",
    "ibam_base    = scale_test(ibam_base,    \"ibam_scaled_score\",        \"imitation_memory\")\n",
    "lswmt_base   = scale_test(lswmt_base,   \"listsorting_scaled_score\", \"list_sorting\")\n",
    "orrt_base    = scale_test(orrt_base,    \"oralreading_scaled_score\", \"oral_reading\")\n",
    "pcps_base    = scale_test(pcps_base,    \"patterncomp_scaled_score\", \"pattern_comparison\")\n",
    "tpvt_base    = scale_test(tpvt_base,    \"pictvocab_scaled_score\",   \"picture_vocab\")\n",
    "\n",
    "# Import ADHD diagnosis data\n",
    "adhd_base = demo_base[[\"subjectkey\", \"cast39_adhd\"]]\n",
    "adhd_base.columns = [\"subjectkey\", \"ADHD\"]\n",
    "adhd_base.loc[:, \"ADHD\"] = adhd_base[\"ADHD\"].replace([\"Missing value\", 777, 999], np.nan)\n",
    "adhd_base.loc[:, \"ADHD\"] = pd.to_numeric(adhd_base[\"ADHD\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a813b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects retained for the analysis: 1238\n"
     ]
    }
   ],
   "source": [
    "# Merge cognitive data fist so this way, we avoid the loss of sujects due to missing data in demographics columns\n",
    "psy_behav = merge_dataframes({\"age_months\" : age, \"ADHD\" : adhd_base, \"card_sort\": dccs_base, \"flanker\" : flanker_base,\n",
    "                               \"imitation_memory\" : ibam_base, \"list_sorting\" : lswmt_base, \"oral_reading\" : orrt_base,\n",
    "                                 \"pattern_comparison\" : pcps_base, \"picture_vocab\" : tpvt_base}, index=\"subjectkey\")\n",
    "\n",
    "# Remove subjects with missing data in any of the columns\n",
    "psy_behav.dropna(inplace=True, axis=0)\n",
    "psy_behav.reset_index(drop=False, inplace=True)\n",
    "print(\"Number of subjects retained for the analysis: {}\".format(psy_behav.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8bb187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary individuals columns from the demographics dataframe and keep only the combined ethnicity column\n",
    "demo_filtered = demo_data[\n",
    "    [\"subjectkey\", \"sex\", \"combine_ethnicity\",\n",
    "     \"parent_ed1\", \"parent_ed2\", \"high_edu\", \"edu_groups\",\n",
    "     \"income\", \"income_groups\", \"handedness\"]\n",
    "].copy()\n",
    "\n",
    "# Concatenating all the dataframes\n",
    "ping_data_gathered = merge_dataframes({\"demo\" : demo_filtered, \"psy_behav\" : psy_behav}, index=\"subjectkey\")\n",
    "\n",
    "# Droping row with NAN in the last 8 colums wich corresponds to the cognitive test scores and ADHD diagnosis\n",
    "ping_data_gathered.dropna(inplace=True, axis=0, subset=ping_data_gathered.columns[-8:], how=\"all\")\n",
    "\n",
    "# Droping 10 subjects with missing data in the handedness column\n",
    "ping_data_gathered.dropna(subset=[\"handedness\"], inplace=True)\n",
    "\n",
    "# Reordering the columns\n",
    "ping_data_gathered = ping_data_gathered[[\"sex\", \"age_months\", \"combine_ethnicity\",\n",
    "                                          \"parent_ed1\", \"parent_ed2\", \"high_edu\", \"edu_groups\",\n",
    "                                          \"income\", \"income_groups\", \"handedness\", \"ADHD\",\n",
    "                                          \"card_sort\", \"flanker\", \"imitation_memory\", \"list_sorting\",\n",
    "                                          \"oral_reading\", \"pattern_comparison\", \"picture_vocab\"]]\n",
    "\n",
    "# Renaming the columns\n",
    "ping_data_gathered.columns = [\"Sex\", \"AgeMonths\", \"Ethnicity\",\n",
    "                                \"Parent_ed1\", \"Parent_ed2\", \"high_edu\", \"edu_groups\",\n",
    "                                \"Income\", \"income_groups\", \"Handedness\", \"ADHD\",\n",
    "                                \"CardSort\", \"Flanker\", \"ImitationMemory\", \"ListSorting\",\n",
    "                                \"OralReading\", \"PatternComparison\", \"PictureVocab\"]\n",
    "\n",
    "# Assert the number of subjects is the same as before -10 subjects with missing data in the handedness column\n",
    "assert ping_data_gathered.shape[0] == psy_behav.shape[0] - 10, \"Number of subjects do not match.\"\n",
    "\n",
    "# Save the final dataframe\n",
    "ping_data_gathered.to_excel(f\"{output_dir}/ping_data_gathered.xlsx\", index=True, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
